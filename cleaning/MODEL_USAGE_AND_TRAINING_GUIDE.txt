================================================================================
FINAL O3 AND NO2 MODEL - USAGE AND TRAINING GUIDE
================================================================================
Generated: 2025-12-04

This guide explains:
1. How to use the final O3 model for predictions
2. How to use the final NO2 model for predictions
3. How to train the same models for different sites

================================================================================
PART 1: USING THE FINAL O3 MODEL
================================================================================

MODEL FILES:
--------------------------------------------------------------------------------
- models/final_o3_model.pkl (or models/enhanced_o3_model.pkl)
- models/final_o3_model.txt (LightGBM text format)

MODEL PERFORMANCE:
--------------------------------------------------------------------------------
- Test R²: 0.8437
- Test RMSE: 4.14
- Test MAE: 3.07
- Improvement over baseline: 84.60%

STEP 1: LOAD THE MODEL
--------------------------------------------------------------------------------
import pickle
import lightgbm as lgb
import pandas as pd
import numpy as np

# Method 1: Load from pickle file
with open('models/final_o3_model.pkl', 'rb') as f:
    o3_model = pickle.load(f)

# Method 2: Load from text file
o3_model = lgb.Booster(model_file='models/final_o3_model.txt')

STEP 2: PREPARE YOUR DATA
--------------------------------------------------------------------------------
Your data must have a 'datetime' column and all required features.

Required Base Columns:
- datetime (datetime format)
- pm2p5, pm10, so2, no2 (pollutants)
- blh_era5, t2m_era5, d2m_era5 (meteorological)
- u10_era5, v10_era5 (wind components)
- tcc_era5, sp (cloud cover, surface pressure)
- solar_elevation, SZA_deg (solar features - CRITICAL for O3)
- O3_target (for lag features - can be NaN for new predictions)

STEP 3: CREATE ALL REQUIRED FEATURES
--------------------------------------------------------------------------------
# Time features
df['year'] = df['datetime'].dt.year
df['month'] = df['datetime'].dt.month
df['day'] = df['datetime'].dt.day
df['hour'] = df['datetime'].dt.hour
df['day_of_week'] = df['datetime'].dt.dayofweek
df['day_of_year'] = df['datetime'].dt.dayofyear
df['is_weekend'] = (df['datetime'].dt.dayofweek >= 5).astype(int)
df['is_weekday'] = (df['datetime'].dt.dayofweek < 5).astype(int)

# Cyclical encoding
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

# Season features
def get_season(month):
    if month in [12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5, 6]:
        return 'summer'
    elif month in [7, 8, 9]:
        return 'monsoon'
    else:
        return 'post_monsoon'

df['season'] = df['month'].apply(get_season)
df['is_winter'] = (df['season'] == 'winter').astype(int)
df['is_summer'] = (df['season'] == 'summer').astype(int)
df['is_monsoon'] = (df['season'] == 'monsoon').astype(int)
df['is_post_monsoon'] = (df['season'] == 'post_monsoon').astype(int)

# Meteorological features
df['wind_speed'] = np.sqrt(df['u10_era5']**2 + df['v10_era5']**2)
df['dewpoint_depression'] = df['t2m_era5'] - df['d2m_era5']
df['relative_humidity_approx'] = 100 * np.exp(-df['dewpoint_depression'] / 5)

# Solar/Photochemical features (CRITICAL for O3)
df['solar_elevation_abs'] = np.abs(df['solar_elevation'])
df['solar_elevation_squared'] = df['solar_elevation']**2
df['solar_elevation_positive'] = np.maximum(0, df['solar_elevation'])
df['is_daytime'] = (df['solar_elevation'] > 0).astype(int)
df['sza_rad'] = np.radians(df['SZA_deg'])
df['cos_sza'] = np.cos(df['sza_rad'])
df['photolysis_rate_approx'] = np.maximum(0, df['cos_sza'])

# Photochemical interaction features
df['temp_solar_elevation'] = df['t2m_era5'] * df['solar_elevation_abs']
df['temp_solar_elevation_squared'] = df['t2m_era5'] * df['solar_elevation_squared']
df['temp_photolysis'] = df['t2m_era5'] * df['photolysis_rate_approx']
df['temp_cos_sza'] = df['t2m_era5'] * df['cos_sza']

# PBL × Solar interactions (CRITICAL)
df['pbl_solar_elevation'] = df['blh_era5'] * df['solar_elevation_abs']
df['pbl_solar_elevation_squared'] = df['blh_era5'] * df['solar_elevation_squared']
df['pbl_photolysis'] = df['blh_era5'] * df['photolysis_rate_approx']
df['pbl_cos_sza'] = df['blh_era5'] * df['cos_sza']
df['pbl_temp'] = df['blh_era5'] * df['t2m_era5']

# Other interactions
df['ventilation_rate'] = df['blh_era5'] / (df['wind_speed'] + 1e-6)
df['pbl_wind_product'] = df['blh_era5'] * df['wind_speed']
df['rh_temp_interaction'] = df['relative_humidity_approx'] * df['t2m_era5']
df['weekend_solar'] = df['is_weekend'] * df['solar_elevation_abs']

# Lag features (1h, 3h, 6h)
for col in ['O3_target', 'no2', 't2m_era5', 'solar_elevation']:
    if col in df.columns:
        for lag in [1, 3, 6]:
            df[f'{col}_lag_{lag}h'] = df[col].shift(lag)

# Rolling mean features (3h, 6h, 12h)
for window in [3, 6, 12]:
    for feat in ['O3_target', 'no2', 't2m_era5']:
        if feat in df.columns:
            df[f'{feat}_rolling_mean_{window}h'] = df[feat].rolling(window=window, min_periods=1).mean()

STEP 4: SELECT FEATURES AND PREPARE FOR PREDICTION
--------------------------------------------------------------------------------
# Get feature list (70 features total)
o3_features = [
    'pm2p5', 'pm10', 'so2', 'no2',
    'blh_era5', 't2m_era5', 'd2m_era5', 'wind_speed', 'u10_era5', 'v10_era5',
    'relative_humidity_approx', 'tcc_era5', 'sp', 'dewpoint_depression',
    'solar_elevation', 'solar_elevation_abs', 'solar_elevation_squared',
    'solar_elevation_positive', 'is_daytime', 'SZA_deg', 'sza_rad', 'cos_sza',
    'photolysis_rate_approx', 'temp_solar_elevation', 'temp_solar_elevation_squared',
    'temp_photolysis', 'temp_cos_sza', 'pbl_solar_elevation',
    'pbl_solar_elevation_squared', 'pbl_photolysis', 'pbl_cos_sza', 'pbl_temp',
    'ventilation_rate', 'pbl_wind_product', 'rh_temp_interaction', 'weekend_solar',
    'O3_target_lag_1h', 'O3_target_lag_3h', 'O3_target_lag_6h',
    'no2_lag_1h', 'no2_lag_3h', 'no2_lag_6h',
    't2m_era5_lag_1h', 't2m_era5_lag_3h', 't2m_era5_lag_6h',
    'solar_elevation_lag_1h', 'solar_elevation_lag_3h', 'solar_elevation_lag_6h',
    'O3_target_rolling_mean_3h', 'O3_target_rolling_mean_6h', 'O3_target_rolling_mean_12h',
    'no2_rolling_mean_3h', 'no2_rolling_mean_6h', 'no2_rolling_mean_12h',
    't2m_era5_rolling_mean_3h', 't2m_era5_rolling_mean_6h', 't2m_era5_rolling_mean_12h',
    'month', 'hour', 'day_of_week', 'is_weekend', 'is_weekday',
    'hour_sin', 'hour_cos', 'month_sin', 'month_cos',
    'is_winter', 'is_summer', 'is_monsoon', 'is_post_monsoon'
]

# Select only features that exist in your dataframe
available_features = [f for f in o3_features if f in df.columns]

# Prepare data
X_pred = df[available_features].copy()

# Convert to numeric
for col in X_pred.columns:
    if X_pred[col].dtype == 'object':
        X_pred[col] = pd.Categorical(X_pred[col]).codes
    elif X_pred[col].dtype == 'bool':
        X_pred[col] = X_pred[col].astype(int)

X_pred = X_pred.select_dtypes(include=[np.number])

# Fill NaN with median (use training data median if available)
for col in X_pred.columns:
    if X_pred[col].isnull().sum() > 0:
        median_val = X_pred[col].median()
        X_pred[col].fillna(median_val, inplace=True)

# Ensure feature order matches model
# The model expects features in a specific order - align with training order
# If you get errors, check the model's feature names:
model_features = o3_model.feature_name()
X_pred = X_pred[[f for f in model_features if f in X_pred.columns]]

STEP 5: MAKE PREDICTIONS
--------------------------------------------------------------------------------
# Make predictions
predictions = o3_model.predict(X_pred, num_iteration=o3_model.best_iteration)

# Add predictions to dataframe
df['O3_predicted'] = predictions

# Note: For real-time predictions, you'll need historical O3_target values
# for lag features. For the first few hours, you may need to use forward-fill
# or initialize with mean values.

================================================================================
PART 2: USING THE FINAL NO2 MODEL
================================================================================

MODEL FILES:
--------------------------------------------------------------------------------
- models/final_no2_model.pkl (contains model + calibrator)
- models/final_no2_model.txt (LightGBM text format)

MODEL PERFORMANCE:
--------------------------------------------------------------------------------
- Uses residual calibration for improved accuracy
- Test R²: ~0.85-0.90 (varies by season)
- Better performance on high-pollution events (peak-weighted training)

STEP 1: LOAD THE MODEL
--------------------------------------------------------------------------------
import pickle
import lightgbm as lgb
import pandas as pd
import numpy as np

# Load model and calibrator
with open('models/final_no2_model.pkl', 'rb') as f:
    no2_model, no2_calibrator = pickle.load(f)

# Or load from text file (no calibrator)
# no2_model = lgb.Booster(model_file='models/final_no2_model.txt')
# no2_calibrator = None  # Will skip calibration

STEP 2: PREPARE YOUR DATA
--------------------------------------------------------------------------------
Required Base Columns:
- datetime (datetime format)
- pm2p5, pm10, so2, no2, pm1, no (pollutants)
- blh_era5, t2m_era5, d2m_era5 (meteorological)
- u10_era5, v10_era5 (wind components)
- tcc_era5, sp (cloud cover, surface pressure)
- solar_elevation (optional, for some features)
- NO2_target (for lag features - can be NaN for new predictions)

STEP 3: CREATE ALL REQUIRED FEATURES
--------------------------------------------------------------------------------
# Time features (same as O3)
df['year'] = df['datetime'].dt.year
df['month'] = df['datetime'].dt.month
df['day'] = df['datetime'].dt.day
df['hour'] = df['datetime'].dt.hour
df['day_of_week'] = df['datetime'].dt.dayofweek
df['day_of_year'] = df['datetime'].dt.dayofyear
df['is_weekend'] = (df['datetime'].dt.dayofweek >= 5).astype(int)
df['is_weekday'] = (df['datetime'].dt.dayofweek < 5).astype(int)

# Cyclical encoding
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

# Season features
def get_season(month):
    if month in [12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5, 6]:
        return 'summer'
    elif month in [7, 8, 9]:
        return 'monsoon'
    else:
        return 'post_monsoon'

df['season'] = df['month'].apply(get_season)
df['is_winter'] = (df['season'] == 'winter').astype(int)
df['is_summer'] = (df['season'] == 'summer').astype(int)
df['is_monsoon'] = (df['season'] == 'monsoon').astype(int)
df['is_post_monsoon'] = (df['season'] == 'post_monsoon').astype(int)

# Meteorological features
df['wind_speed'] = np.sqrt(df['u10_era5']**2 + df['v10_era5']**2)
df['dewpoint_depression'] = df['t2m_era5'] - df['d2m_era5']
df['relative_humidity_approx'] = 100 * np.exp(-df['dewpoint_depression'] / 5)

# Wind components
df['wind_u'] = df['u10_era5']
df['wind_v'] = df['v10_era5']
df['wind_u_abs'] = np.abs(df['u10_era5'])
df['wind_v_abs'] = np.abs(df['v10_era5'])
df['wind_uv_product'] = df['u10_era5'] * df['v10_era5']

# Extended lags (1h, 3h, 6h, 12h, 24h)
for col in ['no2', 'pm2p5', 'pm10', 'so2', 't2m_era5', 'wind_speed']:
    if col in df.columns:
        for lag in [1, 3, 6, 12, 24]:
            df[f'{col}_lag_{lag}h'] = df[col].shift(lag)

# Rolling means (3h, 6h, 12h, 24h)
for window in [3, 6, 12, 24]:
    for feat in ['no2', 'pm2p5', 'pm10']:
        if feat in df.columns:
            df[f'{feat}_rolling_mean_{window}h'] = df[feat].rolling(window=window, min_periods=1).mean()

# PM interactions
if 'pm2p5' in df.columns and 'pm10' in df.columns:
    df['pm25_pm10_ratio'] = df['pm2p5'] / (df['pm10'] + 1e-10)
    df['pm25_pm10_product'] = df['pm2p5'] * df['pm10']
if 'no2' in df.columns and 'pm2p5' in df.columns:
    df['no2_pm25_ratio'] = df['no2'] / (df['pm2p5'] + 1e-10)
    df['no2_pm25_product'] = df['no2'] * df['pm2p5']

# Season-robust features
df['inv'] = df['t2m_era5'] - df['d2m_era5']
df['inversion_strength'] = df['inv']
df['inversion_strength_abs'] = np.abs(df['inv'])
df['ventilation'] = df['blh_era5'] * df['wind_speed']
df['blh_wind'] = df['ventilation']
df['ventilation_rate'] = df['blh_era5'] / (df['wind_speed'] + 1e-6)
df['stability'] = df['inv'] * (1.0 / (df['blh_era5'] + 1e-6))
df['hour_weekend_interaction'] = df['hour'] * df['is_weekend']
df['blh_roll3'] = df['blh_era5'].rolling(window=3, min_periods=1).mean()
df['blh_rolling_mean_3h'] = df['blh_roll3']

# Winter-specific features
df['is_night'] = ((df['hour'] < 7) | (df['hour'] > 20)).astype(int)
df['morning_peak'] = df['hour'].isin([7, 8, 9]).astype(int)
df['blh_inversion'] = df['blh_era5'] * df['inversion_strength']
df['blh_inv'] = df['blh_inversion']
if 'NO2_target' in df.columns:
    df['NO2_target_lag1'] = df['NO2_target'].shift(1)
    df['no2lag_blhlag'] = df['NO2_target_lag1'] * df['blh_era5']
    df['no2_blhlag'] = df['no2lag_blhlag']
df['temperature_roll3'] = df['t2m_era5'].rolling(window=3, min_periods=1).mean()

# Post-monsoon features
df['stubble_burning_flag'] = df['month'].isin([10, 11]).astype(int)
df['diwali_flag'] = (
    ((df['month'] == 10) & (df['day'] >= 20) & (df['day'] <= 24)) | 
    ((df['month'] == 11) & (df['day'] >= 1) & (df['day'] <= 5))
).astype(int)
df['festival_flag'] = df['diwali_flag'].copy()
df['low_wind_flag'] = (df['wind_speed'] < 1.0).astype(int)
df['wind_stagnation_index'] = df['low_wind_flag']
df['low_blh_flag'] = (df['blh_era5'] < 100).astype(int)
df['low_blh'] = df['low_blh_flag']

# BLH-based features
df['blh_lag_1h'] = df['blh_era5'].shift(1)
if 'no2_lag_1h' in df.columns:
    df['blh_no2_lag1_interaction'] = df['blh_era5'] * df['no2_lag_1h']

# Other interactions
df['blh_temp_interaction'] = df['blh_era5'] * df['t2m_era5']
df['hour_temp_interaction'] = df['hour'] * df['t2m_era5']

# Solar (if available)
if 'solar_elevation' in df.columns:
    df['solar_elevation'] = df['solar_elevation']

STEP 4: SELECT FEATURES AND PREPARE FOR PREDICTION
--------------------------------------------------------------------------------
# Get feature list (100+ features)
no2_features = [
    # Core pollutants
    'pm2p5', 'pm10', 'so2', 'no2', 'pm1', 'no',
    # Meteorology
    'blh_era5', 't2m_era5', 'd2m_era5', 'wind_speed', 'u10_era5', 'v10_era5',
    'relative_humidity_approx', 'tcc_era5', 'sp', 'dewpoint_depression',
    # BLH features
    'blh_lag_1h', 'blh_rolling_mean_3h', 'blh_no2_lag1_interaction',
    # Extended lags (add all lag features)
    # Rolling means (add all rolling mean features)
    # PM interactions
    'pm25_pm10_ratio', 'pm25_pm10_product', 'no2_pm25_ratio', 'no2_pm25_product',
    # Wind components
    'wind_u', 'wind_v', 'wind_u_abs', 'wind_v_abs', 'wind_uv_product',
    # Season-robust
    'inv', 'inversion_strength', 'ventilation', 'blh_wind', 'ventilation_rate',
    'stability', 'hour', 'is_weekend', 'hour_weekend_interaction',
    'blh_roll3', 'blh_rolling_mean_3h',
    # Interactions
    'blh_temp_interaction', 'hour_temp_interaction',
    # Time
    'month', 'day_of_week', 'is_weekday', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',
    # Season
    'is_winter', 'is_summer', 'is_monsoon', 'is_post_monsoon',
    # Winter-specific
    'inversion_strength_abs', 'is_night', 'morning_peak',
    'blh_inversion', 'blh_inv', 'no2lag_blhlag', 'no2_blhlag', 'temperature_roll3',
    # Post-monsoon
    'stubble_burning_flag', 'diwali_flag', 'festival_flag',
    'low_wind_flag', 'wind_stagnation_index', 'low_blh_flag', 'low_blh',
    # Solar
    'solar_elevation'
]

# Add all lag and rolling mean features dynamically
for lag in [1, 3, 6, 12, 24]:
    for feat in ['no2', 'pm2p5', 'pm10', 'so2', 't2m_era5', 'wind_speed']:
        no2_features.append(f'{feat}_lag_{lag}h')

for window in [3, 6, 12, 24]:
    for feat in ['no2', 'pm2p5', 'pm10']:
        no2_features.append(f'{feat}_rolling_mean_{window}h')

# Select only features that exist in your dataframe
available_features = [f for f in no2_features if f in df.columns]

# Prepare data
X_pred = df[available_features].copy()

# Convert to numeric
for col in X_pred.columns:
    if X_pred[col].dtype == 'object':
        X_pred[col] = pd.Categorical(X_pred[col]).codes
    elif X_pred[col].dtype == 'bool':
        X_pred[col] = X_pred[col].astype(int)

X_pred = X_pred.select_dtypes(include=[np.number])

# Fill NaN with median
for col in X_pred.columns:
    if X_pred[col].isnull().sum() > 0:
        median_val = X_pred[col].median()
        X_pred[col].fillna(median_val, inplace=True)

# Ensure feature order matches model
model_features = no2_model.feature_name()
X_pred = X_pred[[f for f in model_features if f in X_pred.columns]]

STEP 5: MAKE PREDICTIONS
--------------------------------------------------------------------------------
# Make raw predictions
raw_predictions = no2_model.predict(X_pred, num_iteration=no2_model.best_iteration)

# Apply calibration (if calibrator is available)
if no2_calibrator is not None:
    # Isotonic regression calibration
    calibrated_predictions = no2_calibrator.predict(raw_predictions)
else:
    calibrated_predictions = raw_predictions

# Add predictions to dataframe
df['NO2_predicted'] = calibrated_predictions
df['NO2_predicted_raw'] = raw_predictions  # For comparison

================================================================================
PART 3: TRAINING MODELS FOR DIFFERENT SITES
================================================================================

This section explains how to train the same O3 and NO2 models for a new site.

REQUIREMENTS:
--------------------------------------------------------------------------------
1. Data file: CSV with same column structure as master_site1_final_cleaned.csv
2. Minimum data: At least 2 years of hourly data
3. Required columns: See "Required Base Columns" sections above
4. Python packages: pandas, numpy, lightgbm, sklearn

STEP 1: PREPARE YOUR DATA
--------------------------------------------------------------------------------
# Load your site data
df = pd.read_csv('your_site_data.csv')
df['datetime'] = pd.to_datetime(df['datetime'])
df = df.sort_values('datetime').reset_index(drop=True)

# Ensure you have all required columns:
# - Pollutants: pm2p5, pm10, so2, no2
# - Meteorology: blh_era5, t2m_era5, d2m_era5, u10_era5, v10_era5, tcc_era5, sp
# - Solar: solar_elevation, SZA_deg (CRITICAL for O3)
# - Targets: O3_target, NO2_target

# Check for missing columns
required_cols = ['datetime', 'pm2p5', 'pm10', 'so2', 'no2', 'blh_era5', 
                 't2m_era5', 'd2m_era5', 'u10_era5', 'v10_era5', 
                 'tcc_era5', 'sp', 'solar_elevation', 'SZA_deg',
                 'O3_target', 'NO2_target']

missing_cols = [col for col in required_cols if col not in df.columns]
if missing_cols:
    print(f"WARNING: Missing columns: {missing_cols}")
    print("You may need to obtain these from ERA5 reanalysis or other sources")

STEP 2: CREATE ALL FEATURES
--------------------------------------------------------------------------------
# Copy the feature engineering code from Part 1 (O3) and Part 2 (NO2)
# This creates all time, meteorological, interaction, lag, and rolling features

# For O3 model, use O3 feature engineering
# For NO2 model, use NO2 feature engineering

STEP 3: SPLIT DATA TEMPORALLY
--------------------------------------------------------------------------------
# Use temporal splits (not random) to avoid data leakage
# Recommended splits:
train_mask = (df['datetime'] >= '2020-01-01') & (df['datetime'] <= '2021-12-31')
val_mask = (df['datetime'] >= '2022-01-01') & (df['datetime'] <= '2022-03-31')
test_mask = (df['datetime'] >= '2022-07-01') & (df['datetime'] <= '2022-12-31')

# Adjust dates based on your data availability
# Ensure train period has at least 1.5-2 years of data

STEP 4: TRAIN O3 MODEL
--------------------------------------------------------------------------------
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Get O3 features (use get_o3_features function from train_final_no2_o3.py)
features = get_o3_features(df)

# Prepare data
X_train, y_train, X_val, y_val, X_test, y_test, features = prepare_data(
    df, 'O3_target', features, train_mask, val_mask, test_mask
)

# O3 Model hyperparameters (from O3_MODEL_INFORMATION.txt)
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'num_leaves': 15,
    'max_depth': 5,
    'learning_rate': 0.03,
    'feature_fraction': 0.7,
    'bagging_fraction': 0.7,
    'bagging_freq': 5,
    'min_data_in_leaf': 50,
    'lambda_l1': 1.0,
    'lambda_l2': 1.0,
    'verbose': -1,
    'random_state': 42
}

# Create datasets
train_data = lgb.Dataset(X_train, label=y_train)
val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

# Train model
o3_model = lgb.train(
    params,
    train_data,
    valid_sets=[train_data, val_data],
    valid_names=['train', 'valid'],
    num_boost_round=200,
    callbacks=[
        lgb.early_stopping(stopping_rounds=30, verbose=False),
        lgb.log_evaluation(period=50)
    ]
)

# Evaluate
y_train_pred = o3_model.predict(X_train, num_iteration=o3_model.best_iteration)
y_val_pred = o3_model.predict(X_val, num_iteration=o3_model.best_iteration)
y_test_pred = o3_model.predict(X_test, num_iteration=o3_model.best_iteration)

train_metrics = {
    'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),
    'MAE': mean_absolute_error(y_train, y_train_pred),
    'R2': r2_score(y_train, y_train_pred)
}
val_metrics = {
    'RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred)),
    'MAE': mean_absolute_error(y_val, y_val_pred),
    'R2': r2_score(y_val, y_val_pred)
}
test_metrics = {
    'RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),
    'MAE': mean_absolute_error(y_test, y_test_pred),
    'R2': r2_score(y_test, y_test_pred)
}

print(f"Train RMSE: {train_metrics['RMSE']:.4f}, R²: {train_metrics['R2']:.4f}")
print(f"Val RMSE:   {val_metrics['RMSE']:.4f}, R²: {val_metrics['R2']:.4f}")
print(f"Test RMSE:  {test_metrics['RMSE']:.4f}, R²: {test_metrics['R2']:.4f}")

# Save model
o3_model.save_model('models/your_site_o3_model.txt')
with open('models/your_site_o3_model.pkl', 'wb') as f:
    pickle.dump(o3_model, f)

STEP 5: TRAIN NO2 MODEL
--------------------------------------------------------------------------------
# Get NO2 features (use get_no2_features function from train_final_no2_o3.py)
features = get_no2_features(df, include_winter_features=True)

# Prepare data
X_train, y_train, X_val, y_val, X_test, y_test, features = prepare_data(
    df, 'NO2_target', features, train_mask, val_mask, test_mask
)

# NO2 Model hyperparameters (similar to O3 but may need tuning)
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'num_leaves': 31,  # Slightly higher for NO2
    'max_depth': 7,
    'learning_rate': 0.03,
    'feature_fraction': 0.7,
    'bagging_fraction': 0.7,
    'bagging_freq': 5,
    'min_data_in_leaf': 50,
    'lambda_l1': 1.0,
    'lambda_l2': 1.0,
    'verbose': -1,
    'random_state': 42
}

# Calculate sample weights for peak-weighted training
train_weights = calculate_sample_weights(y_train, percentile=75, weight_factor=4.0)

# Create datasets
train_data = lgb.Dataset(X_train, label=y_train, weight=train_weights)
val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

# Train model
no2_model = lgb.train(
    params,
    train_data,
    valid_sets=[train_data, val_data],
    valid_names=['train', 'valid'],
    num_boost_round=200,
    callbacks=[
        lgb.early_stopping(stopping_rounds=30, verbose=False),
        lgb.log_evaluation(period=50)
    ]
)

# Evaluate
y_train_pred = no2_model.predict(X_train, num_iteration=no2_model.best_iteration)
y_val_pred = no2_model.predict(X_val, num_iteration=no2_model.best_iteration)
y_test_pred = no2_model.predict(X_test, num_iteration=no2_model.best_iteration)

# Train residual calibrator
from sklearn.isotonic import IsotonicRegression
calibrator = IsotonicRegression(out_of_bounds='clip')
calibrator.fit(y_val_pred, y_val)

# Apply calibration
y_test_pred_calibrated = calibrator.predict(y_test_pred)

# Calculate metrics
test_metrics = {
    'RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred_calibrated)),
    'MAE': mean_absolute_error(y_test, y_test_pred_calibrated),
    'R2': r2_score(y_test, y_test_pred_calibrated)
}

print(f"Test RMSE (calibrated): {test_metrics['RMSE']:.4f}, R²: {test_metrics['R2']:.4f}")

# Save model and calibrator
no2_model.save_model('models/your_site_no2_model.txt')
with open('models/your_site_no2_model.pkl', 'wb') as f:
    pickle.dump((no2_model, calibrator), f)

STEP 6: HELPER FUNCTIONS
--------------------------------------------------------------------------------
# You'll need these helper functions from train_final_no2_o3.py:

def get_o3_features(df):
    """O3 feature set"""
    features = []
    features.extend([f for f in ['pm2p5', 'pm10', 'so2', 'no2'] if f in df.columns])
    meteo = ['blh_era5', 't2m_era5', 'd2m_era5', 'wind_speed', 'u10_era5', 'v10_era5',
             'relative_humidity_approx', 'tcc_era5', 'sp', 'dewpoint_depression']
    features.extend([f for f in meteo if f in df.columns])
    # ... (see train_final_no2_o3.py for complete function)

def get_no2_features(df, include_winter_features=True):
    """NO2 feature set"""
    # ... (see train_final_no2_o3.py for complete function)

def prepare_data(df, target_col, features, train_mask, val_mask, test_mask):
    """Prepare data with proper preprocessing"""
    # ... (see train_final_no2_o3.py for complete function)

def calculate_sample_weights(y, percentile=75, weight_factor=4.0):
    """Calculate sample weights for peak-weighted training"""
    threshold = np.percentile(y, percentile)
    weights = np.ones(len(y))
    weights[y > threshold] = weight_factor
    return weights

# Copy these functions from train_final_no2_o3.py or import them

IMPORTANT NOTES FOR TRAINING ON NEW SITES:
--------------------------------------------------------------------------------
1. DATA QUALITY:
   - Ensure data is cleaned (no extreme outliers, missing values handled)
   - Check for sensor drift or calibration issues
   - Verify temporal consistency

2. FEATURE AVAILABILITY:
   - Solar features (solar_elevation, SZA_deg) are CRITICAL for O3
   - If missing, you may need to calculate from latitude/longitude/time
   - ERA5 meteorological data can be downloaded from Copernicus CDS

3. HYPERPARAMETER TUNING:
   - Start with provided hyperparameters
   - If overfitting (train R² >> test R²), increase regularization
   - If underfitting, increase num_leaves or max_depth slightly
   - Use early stopping to prevent overfitting

4. SEASONAL VARIATIONS:
   - O3 patterns vary significantly by season
   - Consider training season-specific models if you have enough data
   - NO2 also shows seasonal patterns (winter peaks)

5. LAG FEATURES:
   - Lag features require historical data
   - For real-time predictions, maintain a rolling window of recent data
   - Initialize first few hours with mean values or forward-fill

6. MODEL VALIDATION:
   - Always use temporal splits (not random)
   - Reserve recent data for testing
   - Check for data leakage (future information in features)

7. PERFORMANCE EXPECTATIONS:
   - O3: R² typically 0.80-0.85 on test set
   - NO2: R² typically 0.85-0.90 on test set
   - Lower performance may indicate:
     * Insufficient data
     * Missing critical features
     * Site-specific factors not captured

8. PRODUCTION DEPLOYMENT:
   - Save feature engineering pipeline
   - Create prediction script that:
     * Loads model
     * Creates all features
     * Handles missing values
     * Makes predictions
   - Monitor model performance over time
   - Retrain periodically with new data

================================================================================
QUICK REFERENCE: FEATURE CHECKLIST
================================================================================

O3 MODEL REQUIRES (70 features):
✓ Core pollutants: pm2p5, pm10, so2, no2
✓ Meteorology: blh_era5, t2m_era5, d2m_era5, wind_speed, u10_era5, v10_era5, etc.
✓ Solar (CRITICAL): solar_elevation, SZA_deg
✓ Photochemical interactions: temp_solar_elevation, pbl_photolysis, etc.
✓ Lag features: O3_target_lag_1h, O3_target_lag_3h, O3_target_lag_6h
✓ Rolling means: O3_target_rolling_mean_3h, etc.
✓ Time features: month, hour, day_of_week, cyclical encodings
✓ Season features: is_winter, is_summer, is_monsoon, is_post_monsoon

NO2 MODEL REQUIRES (100+ features):
✓ Core pollutants: pm2p5, pm10, so2, no2, pm1, no
✓ Meteorology: blh_era5, t2m_era5, d2m_era5, wind_speed, etc.
✓ Extended lags: no2_lag_1h through no2_lag_24h
✓ Rolling means: no2_rolling_mean_3h through no2_rolling_mean_24h
✓ Season-robust features: inversion_strength, ventilation, stability
✓ Winter-specific: blh_inversion, morning_peak, is_night
✓ Post-monsoon: stubble_burning_flag, diwali_flag, low_wind_flag
✓ Time and season features

================================================================================
TROUBLESHOOTING
================================================================================

PROBLEM: "Feature mismatch" error
SOLUTION: Ensure feature order matches training order. Use model.feature_name() to check.

PROBLEM: Poor predictions (low R²)
SOLUTION: 
- Check data quality and missing values
- Verify all critical features are present (especially solar for O3)
- Ensure sufficient training data (at least 1.5-2 years)
- Check for data leakage or temporal issues

PROBLEM: Lag features are NaN
SOLUTION: 
- Ensure data is sorted by datetime
- For first few rows, use forward-fill or mean values
- Maintain rolling window of recent data for real-time predictions

PROBLEM: Model file not found
SOLUTION:
- Check file paths are correct
- Use absolute paths if relative paths fail
- Ensure model files are in 'models/' directory

PROBLEM: Memory errors during training
SOLUTION:
- Reduce number of features
- Use smaller data splits
- Increase system RAM or use cloud computing

================================================================================
END OF GUIDE
================================================================================

For questions or issues, refer to:
- O3_MODEL_INFORMATION.txt (detailed O3 model info)
- O3_MODEL_FEATURES.txt (complete O3 feature list)
- train_final_no2_o3.py (complete training code)

